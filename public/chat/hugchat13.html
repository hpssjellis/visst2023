
<!-- helps to convert markdown output from the chat into HTML -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/showdown/2.1.0/showdown.js"></script> 

<!-- polyfill for firefox + import maps   
<script src="https://unpkg.com/es-module-shims@1.7.0/dist/es-module-shims.js"></script> 
-->

<!-- Huggingface main inport  -->
<script type="importmap">
   {"imports": {"@huggingface/inference": "https://cdn.jsdelivr.net/npm/@huggingface/inference@1.7.1/+esm"}}
</script>

<body onload="{
   myStorage = localStorage.getItem('myStoredToken')
   if(myStorage  != null){
      document.getElementById('myTokenID').value = myStorage 
    }
}">
<h6> version 0.7.1-28</h6>
<h3 align=center>Hugginface single page Javascript Open Souce Chat</h3>

Prompt:<input type="button" value="Clear Prompt" onclick="{  
    document.getElementById('myPromptAndInput').value = ''
}"> <br>
<textarea id="myPromptAndInput" rows=10 cols=60>What is a large language model</textarea> <br>
 <a href="https://huggingface.co/settings/tokens"> Token Signup</a>, Token ID (faster): <input type="password" value="" id="myTokenID"> 
<input type="button" value="Store Token Locally" onClick="{
   localStorage.setItem('myStoredToken', document.getElementById('myTokenID').value)
   document.getElementById('myOutputHtml').innerHTML = 'Latest Token has been stored containing these last 4 characters: ' +
       document.getElementById('myTokenID').value.slice(-4)
}"><br><br>
 <a href="https://huggingface.co/models?pipeline_tag=text2text-generation&sort=likes"> HF Models at</a>. 
	This Model: <input type="text" size=50 value="OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5" id="myModel">  <br>


<input type="button" value="Run Huggingface Chat" onclick="{  
   launch()
}"> 

<input type=button value="Convert Output to HTML" onClick="{
   let converter = new showdown.Converter()
   document.getElementById('myOutputHtml').innerHTML  = converter.makeHtml(document.getElementById('myPromptAndInput').value)
}"><br>
HTML Output: <span id="myOutputHtml">...</span> <br>
<input type="number" id="myParamTemp" value="0.95" placeholder="0.95" min="0.001" title="temperature: low=random, high=precise" max="1">



<script type="module">
import { HfInference } from "@huggingface/inference";
let running = false;
async function launch() {
if (running) {return;}
  running = true;
  try {
    const hf = new HfInference(document.getElementById("myTokenID").value.trim() || undefined);
	  
    // models at         https://huggingface.co/models?pipeline_tag=text2text-generation&sort=likes
    // some need a paid account
    //const model =  "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"; //meta-llama/Llama-2-70b-chat-hf";  //document.getElementById("model").value.trim();  // set this
    const model =  document.getElementById("myModel").value.trim();  // set the model
    //document.getElementById("myModel").innerHTML = `<b>${model}</b>`;
		
    const prompt = document.getElementById("myPromptAndInput").value.trim();
    document.getElementById("myOutputHtml").textContent  = "";
    for await (const output of hf.textGenerationStream({model,inputs: prompt,parameters: { 
	    temperature: parseInt(document.getElementById("myParamTemp").value),
	    top_p: 0.95,
	    repetition_penalty: 1.2, 
	    top_k: 50, 
	    truncate: 1000,
	    max_new_tokens: 250
       }}, {use_cache: false})) {
      document.getElementById("myPromptAndInput").value += output.token.text;
     // document.getElementById("myOutputHtml").textContent  += output.token.text;
    }
  } catch (err) {
      alert("Error: " + err.message);
  } 
    finally {running = false;}
  }
  window.launch = launch;

/*
      "temperature": 0.9,           // control the randomness of predictions
      "top_p": 0.95,                    // control the range of tokens based on their cumulative probability
      "repetition_penalty": 1.2,   // discounts the probability of tokens that already appeared 
      "top_k": 50,                       // top list of number of words to choose from
      "truncate": 1000,               // ?????
      "max_new_tokens": 1024

*/
	
</script>
